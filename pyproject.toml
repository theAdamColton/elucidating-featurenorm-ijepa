[project]
name = "ijepa-tanh"
version = "0.1.0"
description = "Elucidating the role of feature normalization in IJEPA"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "accelerate>=1.6.0",
    "einx>=0.3.0",
    "flash-attn",
    "jsonargparse[signatures]>=4.38.0",
    "matplotlib>=3.10.1",
    "numpy>=2.2.5",
    "pillow-simd>=9.5.0.post2",
    "tensorset==0.4.5",
    "torch>=2.7.0",
    "torch-pca>=1.1.0",
    "torchvision>=0.22.0",
    "transformers>=4.51.3",
    "wandb[media]>=0.19.10",
    "webdataset>=0.2.111",
]

[dependency-groups]
dev = [
    "bpython>=0.25",
]

# This is hardcoded to cuda12.8 and torch 2.7 and python 3.12
[tool.uv.sources]
flash-attn = { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.1.0/flash_attn-2.7.4+cu128torch2.7-cp312-cp312-linux_x86_64.whl" }
