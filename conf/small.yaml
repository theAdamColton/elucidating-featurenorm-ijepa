# batch size 384, about 1415 samples per batch
# 350 epochs, with 894 steps per epoch, a total of about 313000 steps
conf:
  mode: train

  should_compile: true
  dtype: bfloat16
  device: cuda

  steady_lr: 1e-3
  num_lr_steady_steps: 250000
  num_lr_cooldown_steps: 50000
  ema_beta_steady_steps: 300000
  num_epochs: 350
  end_lr: 7e-5

  batch_size: 384
  num_workers: 4

  num_register_tokens: 0
  validate_every_num_epochs: 50

  model:

    encoder:
      input_size: 768
      num_transformer_blocks: 12
      block_config:
        embed_dim: 384
        attention_config:
          embed_dim: 384
          head_dim: 64
          num_attention_heads: 6
          should_use_qk_norm: true
        norm_mode: layernorm
        mlp_mode: vanilla
      norm_out_mode: dyntanh
      norm_elementwise_affine: false

    predictor:
      input_size: 384
      num_transformer_blocks: 6
      block_config:
        embed_dim: 256
        attention_config:
          embed_dim: 256
          head_dim: 64
          num_attention_heads: 4
          should_use_qk_norm: true
        norm_mode: layernorm
        mlp_mode: vanilla
      norm_out_mode: layernorm

    predictor_batch_repeat: 4
    target_norm_mode: disabled
    should_predict_from_all_target: false
    should_attempt_mask_dropping: true
    predictor_context_capacity: 0.25
    predictor_target_capacity: 0.25

    sample_predictor_context_with_replacement: true
    sample_predictor_targets_with_replacement: true

  context_target_dataset:
    min_context_capacity: 0.25
    max_context_capacity: 0.5
    absolute_max_context_capacity: 0.5
    mask_window_size: 2
    max_side_length: 256
    min_side_length: 64

